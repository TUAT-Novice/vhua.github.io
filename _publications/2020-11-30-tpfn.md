---
title: "TPFN: Applying Outer Product Along Time to Multimodal Sentiment Analysis Fusion on Incomplete Data"
collection: publications
permalink: /publication/2020-11-30-tpfn
excerpt: 'This paper proposes a TPFN model for multimodal sentiment analysis fusion on incomplete data.'
date: 2020-11-30
venue: 'European Conference on Computer Vision (ECCV 2020)'
slidesurl: 'https://suilf.github.io/assets/publications/ECCV2020_LiChao/ECCV2020_slides.pdf'
paperurl: 'https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4631_ECCV_2020_paper.php'
codeurl: 'https://qibinzhao.github.io/codes/'
citation: 'Binghua Li, Chao Li, Feng Duan, Ning Zheng and Qibin Zhao. TPFN: Applying Outer Product Along Time to Multimodal Sentiment Analysis Fusion on Incomplete Data. 16th European Conference on Computer Vision (ECCV 2020). Springer International Publishing, 2020: 431-447.'
pubtype: 'conference'
---

Multimodal sentiment analysis (MSA) has been widely investigated in both computer vision and natural language processing. However, studies on the imperfect data especially with missing values are still far from success and challenging, even though such an issue is ubiquitous in the real world. Although previous works show the promising performance by exploiting the low-rank structures of the fused features, only the first-order statistics of the temporal dynamics are concerned. To this end, we propose a novel network architecture termed Time Product Fusion Network (TPFN), which takes the high-order statistics over both modalities and temporal dynamics into account. We construct the fused features by the outer product along adjacent time-steps, such that richer modal and temporal interactions are utilized. In addition, we claim that the low-rank structures can be obtained by regularizing the Frobenius norm of latent factors instead of the fused features. Experiments on CMU-MOSI and CMU-MOSEI datasets show that TPFN can compete with state-of-the art approaches in multimodal sentiment analysis in cases of both random and structured missing values.
